#! /usr/bin/python

import os, subprocess, shlex, re, stat, sys, time, json, traceback, uuid, socket

def help(args=['--h']):
    txt = '''
dis -- deploy in stages

Use rsync to push data out to a deployment hierarchy, with delays between
each stage in the hierarchy to allow us to detect and react to possible
problems.

dis queue <src> [<src>...] <target:path>

    Submit a new deploy job.
    
    - src: one or more local files/folder to deploy via rsync. This arg is
          repeatable so shell globbing is usable.
    - target:path: an rsync-style host:path pair, except that the host part
          is a deployment target defined in targets.json; currently, valid
          values are %s.
    
dis more [--quiet]

    Do any pending work to advance deploy jobs to the next stage, if their
    waiting period has elapsed. Do nothing if queue is empty. Suitable for
    scheduling as a command in cron.
    
dis status [<bucket>]

    Show status on jobs. By default, all jobs that are not at a terminal
    status are displayed; if a bucket such as "failed" or "done" is specified,
    the report scope is narrowed accordingly.
    
dis reset <job_id>

    Clear an incomplete job's state so it's ready to run again. Job id can be
    any convenient prefix, instead of the full value.    
    
dis age <job_id>

    Expire any local timeouts on a job, so it will progress immediately with
    the next "more" command.
    
dis stop <job_id>

    Interrupt a job. No rollback is performed, but any future work is
    canceled. Job id can be any convenient prefix, instead of the full
    value.

''' % make_target_list()
    if (len(args) != 1 or not help_pat.match(args[0])):
        complain('Error in cmdline syntax.\n\n' + txt)
    print(txt)
    sys.exit(0)
    

this_script = os.path.normpath(os.path.realpath(os.path.abspath(__file__)))
my_folder = os.path.dirname(this_script)
targets_path = os.path.join(my_folder, 'targets.json')
help_pat = re.compile(r'^(/|--?)(\?|h(elp)?)$', re.I)
remote_install_path = '~/deploy-in-stages'
dict_type = type({})
busy_suffix = '.busy'


with open(targets_path, 'r') as f:
    targets = json.load(f)['targets']


def is_string(x):
    return isinstance(x, str) or isinstance(x, unicode)


def is_dict(x):
    return bool(getattr(x, 'keys', None)) and bool(getattr(x, 'items', None))


def is_list(x):
    return bool(getattr(x, 'append', None)) and bool(getattr(x, 'reverse', None))    


def friendly_time(t, zero=None):
    if zero and (t == 0):
        return zero
    return time.asctime(time.localtime(t))


_hostname = None
def get_my_hostname():
    '''Provide a hostname that's had redundant suffixes removed.'''
    global _hostname
    if not _hostname:
        _hostname = socket.getfqdn()
        if _hostname.endswith('.es.bluecoat.com'):
            _hostname = _hostname[:-16]
    return _hostname
    

def complain(msg):
    sys.stderr.write('\n' + msg + '\n\n')
    sys.exit(1)
    

class json_wrapper(object):
    '''
    Make it possible to use dotted notation (x.y) in addition
    to the clumsier dict notation (x['y']) on dicts built by
    the json library.
    '''
    def __init__(self, raw):
        # This next line prevents infinite recursion that would happen if we
        # just wrote self.raw = raw. (If we did that, "self.raw" would cause
        # __getattr__ to be called, since "raw" isn't yet in self.__dict__..)
        super(json_wrapper, self).__setattr__('raw', raw)
    def __getattr__(self, name):
        # This line doesn't recurse because 'raw' is already in self.__dict__.
        raw = self.raw
        if name == 'raw':
            return raw
        if name in raw:
            answer = raw[name]
            if type(answer) == dict_type:
                return json_wrapper(answer)
            return answer
        return getattr(raw, name)
    def __len__(self):
        return len(self.__dict__['raw'])
    def __repr__(self):
        return self.__dict__['raw'].__repr__()
    def __str__(self):
        return self.__dict__['raw'].__str__()
    def __setattr__(self, name, value):
        if name in self.__dict__:
            self.__dict__[name] = value
        else:
            self.raw[name] = value


class source_item:
    def __init__(self, path_or_dict, size=None, mtime=None, auto_stat=True):
        if isinstance(path_or_dict, dict):
            self.path = path_or_dict['path']
            if 'size' in path_or_dict:
                size = path_or_dict['size']
            if 'lastmod_time' in path_or_dict:
                mtime = path_or_dict['lastmod_time']
        else:
            self.path = path_or_dict
        if auto_stat:
            self.path = os.path.normpath(os.path.realpath(os.path.abspath(os.path.expanduser(self.path))))
            if (size is None or mtime is None):
                try:
                    info = os.stat(self.path)
                    if stat.S_ISDIR(info[stat.ST_MODE]):
                        size = mtime = -1
                        if not self.path.endswith('/'):
                            self.path += '/'
                    else:
                        if size is None:
                            size = info[stat.ST_SIZE]
                        if mtime is None:
                            mtime = info[stat.ST_MTIME]
                except:
                    pass
        self.size = size
        self.lastmod_time = mtime
        
    def __eq__(self, other):
        return self.path == other.path \
            and self.size == other.size \
            and self.lastmod_time == other.lastmod_time
        
    def __ne__(self, other):
        return not self == other
        
    def is_folder(self):
        return size == -1


class buckets:
    queued = 'queued'
    pending = 'pending'
    done = 'done'
    failed = 'failed'
    stopped = 'stopped'
    
    active = [queued, pending]
    terminal = [done, failed, stopped]
    all = active + terminal
        
    @staticmethod
    def ensure_exists(bucket):
        b = buckets.folder_for(bucket)
        if not os.path.isdir(b):
            os.makedirs(b)
    
    @staticmethod
    def folder_for(bucket):
        return os.path.join(my_folder, bucket)

    @staticmethod
    def ids_of_jobs_in(bucket):
        folder = buckets.folder_for(bucket)
        if os.path.isdir(folder):
            items = [x for x in os.listdir(folder) if not x.endswith('.busy')]
            return items
        return []
    
    @staticmethod
    def path_for_job(bucket, job_id):
        return os.path.join(buckets.folder_for(bucket), job_id)
    
    
class retry_info:
    def __init__(self, when, count=0):
        self.next_check_time = when
        self.count = count
        

class custom_encoder(json.JSONEncoder):
    '''
    Extend json serialization so it supports the source_item and job classes.
    '''
    def default(self, obj):
        if isinstance(obj, source_item) or isinstance(obj, retry_info):
            return obj.__dict__
        elif isinstance(obj, job):
            return obj.raw
        return json.JSONEncoder.default(self, obj)


class job(json_wrapper):
    '''
    Manage all the state for a job, preserving important invariants. Allow
    the job to be loaded from and saved to json, and provide some utility
    methods.
    '''
    
    def __init__(self, raw):
        json_wrapper.__init__(self, raw)
        # These two attributes don't serialize to json, and they should
        # not be in the "raw" dict. However, they should always exist.
        self.__dict__['_busy'] = False
        self.__dict__['_path'] = None
        if 'sources' in self.raw:
            x = self.sources
            if x and not isinstance(x[0], source_item):
                self.sources = [source_item(x) for x in self.sources]
        if 'retries' in self.raw:
            r = self.raw['retries']
            if (type(r) == dict_type) and r:
                for key in r.keys():
                    item = r[key]
                    if not isinstance(item, retry_info):
                        r[key] = retry_info(item['next_check_time'], item['count'])
                        
    def short_id(self):
        return self.uuid[:4]
        
    def host_is_terminal(self, host):
        for bucket in buckets.terminal:
            if host in self.raw[bucket]:
                return True
        return False
    
    def put_host_in(self, host, bucket_list):
        r = self.__dict__['raw']['retries']
        if host in r:
            del r[host]
        dicts = [self.failed, self.pending, self.done]
        for d in dicts:
            if not (d is bucket_list):
                if host in d:
                    d.remove(host)
        bucket_list.append(host)
        
    def host_should_be_queued(self, host):
        return host not in self.pending and not self.host_is_terminal(host)
        
    def choose_next_check_time(self):
        '''
        Picks the next time that it will be useful to check this job, sets
        the .next_check_time member, and returns it.
        '''
        value = time.time() + 60
        if self.has_terminated():
            value = 0
        elif self.is_stalled():
            value = None
            for host in self.hosts:
                retry_info = job.retries.get(host, None)
                if self.host_should_be_queued(host):
                    if retry_info:
                        nt = retry_info.next_check_time
                        if value is None or nt < value:
                            value = nt
                else:
                    del self.retries[host]
        self.next_check_time = value
        return value
    
    def move_to(self, bucket):
        if self.get_bucket() == bucket:
            return
        
        new_folder = buckets.folder_for(bucket)
        if not os.path.isdir(new_folder):
            os.makedirs(new_folder)
            
        new_path = os.path.join(new_folder, self.uuid)
        if self._busy:
            new_path += busy_suffix
            
        os.rename(self._path, new_path)
        self.set_path(new_path)
        self.bucket = bucket
        self.log('Status is now %s.' % bucket)
        print('Job %s status is %s.' % (self.short_id(), bucket))
        return new_path
    
    def has_terminated(self):
        '''
        Return True if we've completed all work--even if we haven't yet had
        our official bucket updated.
        '''
        if self.get_terminal_host_count() == len(self.hosts):
            return True
        b = self.get_bucket()
        #print('bucket for %s = %s' % (self.get_path(), b))
        return b in buckets.terminal
    
    def bucket_list_by_name(self, name):
        if name == buckets.done:
            return self.done
        elif name == buckets.failed:
            return self.failed
        elif name == buckets.pending:
            return self.pending
        elif name == buckets.stopped:
            return self.stopped
        elif name == buckets.queued:
            return self.queued
    
    def get_terminal_host_count(self):
        '''
        Return the number of hosts that have reached terminal status.
        '''
        return len(self.failed) + len(self.done) + len(self.stopped)       
    
    def is_stalled(self):
        '''
        Return True if we can't mature at the normal time, because all hosts
        that aren't in a terminal state have pending retries.
        '''
        if (not self.retries) or self.has_terminated():
            return False
        return len(self.retries) + self.get_terminal_host_count() >= len(self.hosts)

    def is_busy(self):
        return self._busy
    
    def set_busy(self, value):
        '''
        Mark a job as busy or not-busy, to reflect whether it's being actively
        processed.
        '''
        # If no change, just return current path
        if value == self._busy:
            return self._path
        if self._busy:
            new_path = self._path[:-len(busy_suffix)]
        else:
            new_path = self._path + busy_suffix
        os.rename(self._path, new_path)
        self.set_path(new_path)
        return self._path
    
    def schedule_retry(self, host):
        x = self.__dict__['raw']['retries']
        r = x.get(host, None)
        if not r:
            r = retry_info(0)
            x[host] = r
        r.count += 1
        if r.count < 5:
            r.next_check_time = time.time() + (r.count**3 * 60)
        else:
            del x[host]
            r = None
        return r
    

    def get_bucket(self):
        x = self._path
        if x:
            return os.path.basename(os.path.split(x)[0])
        
    def get_path(self):
        return self._path
    
    def set_path(self, path):
        self.__dict__['_path'] = os.path.abspath(path)
        self.__dict__['_busy'] = path.endswith(busy_suffix)
    
    @staticmethod
    def empty():
        raw = job._get_empty_raw()
        return job(raw)

    @staticmethod
    def _get_empty_raw():
        '''Build a dict that contains all the attributes of a canonical job.'''
        return {
            'uuid': uuid.uuid1().hex,
            'bucket': '',
            'sources': [],
            'target': '',
            'queue_time': time.time(),
            'last_check_time': 0,
            'next_check_time': 0,
            'pending': [],
            'done': [],
            'failed': [],
            'events': [],
            'stopped': [],
            'hosts': [],
            'retries': {},
            'aged': 0
        }
    
    prototype = None
    
    def _upgrade(self):
        '''
        If we add new members to a job, old jobs might become unparseable.
        By calling this method, that problem should go away.
        '''
        x = job.prototype.raw
        y = self.raw
        for key in x.keys():
            if key not in y:
                y[key] = x[key]
        
    @staticmethod
    def load(path):
        with open(path, 'r') as f:
            j = job(json.load(f))
            # If this is an old job, give it default attribs for new properties.
            j._upgrade()
            j.set_path(path)
            return j

    def save(self, path=None):
        if path is None:
            path = self._path
        with open(path, 'w') as f:
            json.dump(self.raw, f, indent=2, cls=custom_encoder)
            self.set_path(path)
            
    def report(self, event):
        print(event)
        self.log(event)
        
    def log(self, event):
        self.events.insert(0, 'On %s at %s: %s' % (get_my_hostname(), time.asctime(), event))
        
job.prototype = job.empty()


rsync_output_pat = re.compile(r'\s*[dwrx-]+ +([0-9,]+)')

def get_remote_size(remote_path):
    cmd = 'rsync %s' % remote_path
    try:
        stdout = subprocess.check_output(shlex.split(cmd))
        match = rsync_output_pat.match(stdout)
        if match:
            return int(match.group(1).replace(',', ''))
    except:
        pass
    return -1


def queue(sources, rsync_spec):
    try:
        if ':' not in rsync_spec:
            complain('Target should be an rsync-style host:path couplet.')
            
        node, path = rsync_spec.split(':')
        
        if not path.startswith('/') and not path.startswith('~/'):
            complain('Remote path must either be absolute or relative to ~/.')
            
        if is_string(sources):
            sources = [sources]
            
        j = job.empty()
        j.sources = [source_item(x) for x in sources]
        j.target = rsync_spec
        j.bucket = buckets.queued
        
        # Convert the target name into a list of hosts. We do this as we queue,
        # so that if our target gets redefined before we're done, the scope of
        # the job doesn't creep. This also forces us to prove that the target
        # name is valid.
        tgt, remote_path = rsync_spec.split(':')
        tgt = targets.get(tgt, {tgt: None})
        j.hosts = tgt.keys()
        
        buckets.ensure_exists(buckets.queued)
        job_path = buckets.path_for_job(buckets.queued, j.uuid)
        j.save(job_path) 
        print('Job %s is queued.' % j.short_id())
        more(False)
        
    except SystemExit:
        raise
    
    except:
        complain(traceback.format_exc())
        

def ignore_no_such_folder(errors):
    if 'No such file or directory' in errors:
        return None
    return errors

def collect_results(j, host, updated_hosts):
    # If the remote host is sending us results, make sure it's running latest script.
    if host not in updated_hosts:
        if rsync(this_script, '%s:%s/' % (host, remote_install_path)):
            updated_hosts[host] = 1

    terminal_count = 0
    for bucket in buckets.terminal:
        terminal_file = '%s:%s/%s/%s' % (host, remote_install_path, bucket, j.uuid)
        local_file = '/tmp/%s' % j.uuid
        if rsync(terminal_file, local_file, error_filter=ignore_no_such_folder):
            terminal_count += 1
            remote_job = job.load(local_file)
            os.remove(local_file)
            if remote_job.events:
                for e in remote_job.events:
                    j.events.insert(0, e)
            lst = j.bucket_list_by_name(bucket)
            if host not in lst:
                j.put_host_in(host, lst)
                j.report('Host %s is %s.' % (host, bucket))
    if not terminal_count:
        j.log('No results yet from %s.' % host)
    

def distribute(j, host, remote_path, cascade_target, tallies, updated_hosts):
    try:
        
        # If this host is going to run a dis job of its own, make sure its dis script
        # is up-to-date. This constitutes a sort of autoupdate feature; it allows us
        # to run with confidence, even if only the top of a distribution hierarchy has
        # the latest code when we begin.
        if cascade_target:
            if host not in updated_hosts:
                if rsync(this_script, '%s:%s/' % (host, remote_install_path)):
                    updated_hosts[host] = 1
        
        # Now push the data.
        if rsync(j.sources, '%s:%s' % (host, remote_path)):
            j.log('Successfully pushed data to %s:%s.' % (host, remote_path))
            tallies.payloads += 1
            
            # Now see if we need to define and push a job onto the remote host,
            # to cascade additional distribution work.
            if cascade_target:
                
                # Rewrite sources so they are stated in terms of the remote host's fs.
                revised_sources = [
                    source_item(os.path.join(remote_path, os.path.basename(x.path)),
                        x.size,
                        None, # we don't know what mtime we'll have remotely
                        auto_stat = False
                    ) for x in j.sources
                ]
                
                child_job = job.empty()
                child_job.uuid = j.uuid
                child_job.target = '%s:%s' % (cascade_target, remote_path)
                child_job.sources = revised_sources
                child_job.next_check_time = 0
                child_job.log('Delegated a portion of job %s.' % j.short_id())
                
                # We put the job in a 'queued' subdir because if the queued
                # subdir doesn't yet exist remotely, we'll end up putting
                # a raw file in the wrong place, unless we replicate a whole
                # folder...
                repl = '/tmp/queued'
                if not os.path.isdir(repl):
                    os.makedirs(repl)
                else:
                    items = [os.path.join(repl, x) for x in os.listdir(repl)]
                    for x in items:
                        os.remove(x)
                fname = '%s/%s' % (repl, j.uuid)
                child_job.save(fname)
                
                # Send the job to the remote machine.
                succeeded = True
                try:
                    if rsync(repl, '%s:%s/' % (host, remote_install_path)):
                        j.report('Queued delegated portion of job %s on %s.' % (j.short_id(), host))
                        j.put_host_in(host, j.pending)
                    else:
                        j.log('Unable to queue remote job on %s.' % host)
                        if j.schedule_retry(host):
                            tallies.retries += 1
                            j.report('Attempt to queue delegated portion of job %s on %s failed, but retry is scheduled.' % (j.short_id(), host))
                        else:
                            j.report('Attempt to queue job %s on %s definitively failed and will not be retried.' % (j.short_id(), host))
                            j.put_host_in(host, j.failed)
                        succeeded = False
                finally:
                    os.remove(fname)
                return succeeded
                
            else:
                j.log('No delegation necessary; %s is a terminal distribution point.' % host)
                j.put_host_in(host, j.done)
                return True
            
        else:
            j.log('Unable to push data to %s.' % host)
            if j.schedule_retry(host):
                j.report('Attempt to push data to %s failed, but retry is scheduled.' % host)
                tallies.retries += 1
            else:
                j.report('Attempt to push data to %s definitively failed and will not be retried.' % host)
                j.put_host_in(host, j.failed)
            return False
        
    except Exception:
        j.report(traceback.format_exc())
        j.put_host_in(host, j.failed)
        return False
    
def rsync(src, tgt=None, error_filter=None):
    # src can be a string or a list of objects that contain path+lastmod+size.
    # However, we need it to be a list below.
    if type(src) != type([]):
        src = [src]
        
    src_is_local = None
    
    for item in src:
        recurse_switch = ''
        
        is_obj = isinstance(item, source_item)
        
        # If we haven't figured out whether we're dealing with a local source
        # yet, do so now.
        if src_is_local is None:
            src_is_local = is_obj or (':' not in item)
            
        src_path = item
        if src_is_local:
            # If we were given an object that has lastmod and size attribs,
            # verify that we're still dealing with the same thing.
            if is_obj:
                changes = []
                try:
                    current_item = source_item(item.path)
                    if item.lastmod_time != current_item.lastmod_time:
                        changes.append('lastmod is different (%s --> %s)' %
                                       (friendly_time(item.lastmod_time),
                                        friendly_time(current_item.lastmod_time)))
                    if item.size != current_item.size:
                        changes.append('size is different (%d --> %d)' %
                                       (item.size, current_item.size))
                    #print("ready to make the attempt")
                except:
                    changes.append(traceback.format_exc())
                if changes:
                    raise Exception('%s has changed since it was queued (%s).' % ', '.join(changes))
                src_path = item.path
                
        if os.path.isdir(src_path):
            recurse_switch = '-r '
        cmd = 'rsync %s%s %s' % (recurse_switch, src_path, tgt)
        #print(cmd)
        errors = None
        try:
            proc = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
            stdout, stderr = proc.communicate()
            if proc.returncode:
                errors = ('%s\n%s\n%s' % (cmd, stdout, stderr)).strip().replace('\n\n', '\n')
        except subprocess.CalledProcessError as e:
            errors = '%s\n%s' % (cmd, e.output).strip()
        if errors:
            if error_filter:
                errors = error_filter(errors)
            if errors:
                print(errors)
            return False
    return True


def parse_cascade_spec(spec, host):
    delay = 0
    tgt = None
    if spec:
        items = spec.split(' ')
        for item in items:
            try:
                delay = int(item)
            except:
                tgt = item
    return tgt, delay


def handle_bucket(bucket, tallies, updated_hosts):
    
    for id in buckets.ids_of_jobs_in(bucket):
        try:
            job_path = buckets.path_for_job(bucket, id)
            
            # Make sure we can load the .json for the job
            try:
                j = job.load(job_path)
            except:
                tallies.skipped += 1
                traceback.print_exc()
                print('Ignored %s/%s; not a valid json job.' % (bucket, id))
                continue

            # Is this job mature (ready to be checked)?
            now = time.time()
            if j.aged or (j.next_check_time <= now):
                
                # Just in case, temporarily change the filename to show that
                # we're working with the job. This will prevent multiple copies
                # of the script that are running in parallel from stepping on
                # one another's toes.
                try:
                    #print("marking %s as busy" % job_path)
                    j.set_busy(True)
                except:
                    # If we failed to rename, log issue and continue.
                    traceback.print_exc()
                    print('Unable to mark %s busy. Is another copy of this script running?' % job_path)
                    continue
                
                j.last_check_time = now
                
                try:
                    tgt, remote_path = j.target.split(':')
                    # Convert named target into a mapping to a cascade spec
                    target = targets.get(tgt, {tgt: None})
                    collected_count = 0
                    
                    # If our list of hosts is empty, and this is a queued job, it likely means
                    # that the job was delegated from a remote supervisor. It's up to us to
                    # expand the host list based on the target name.
                    if (not j.hosts) and bucket == buckets.queued:
                        j.hosts = target.keys()
                        if not j.hosts:
                            j.log('No hosts are defined. Job is a no-op...')
                    
                    for host in j.hosts:
                        #print("handling host %s" % host)
                        if host in j.done or host in j.failed or host in j.stopped:
                            # We don't need to react to stuff we already got to a
                            # terminal status.
                            pass
                        elif host in j.pending:
                            print('Polling %s for results on job %s...' % (host, j.short_id()))
                            if collect_results(j, host, updated_hosts):
                                tallies.delegates_changed += 1
                        else:
                            cascade_target, cascade_delay = parse_cascade_spec(target[host], host)
                            host_mature_time = j.queue_time + (60 * cascade_delay)
                            retry_info = j.retries.get(host, None)
                            if retry_info:
                                host_mature_time = retry_info.next_check_time
                            if (host_mature_time <= now) or j.aged:
                                distribute(j, host, remote_path, cascade_target, tallies, updated_hosts)
                            else:
                                print("%s won't be ready for job %s until %s." % (host, j.short_id(), friendly_time(host_mature_time)))
                                
                    # Did we reach a terminal status on overall job?
                    if j.has_terminated():
                        if j.failed:
                            tallies.failed += 1
                            new_bucket = buckets.failed
                        else:
                            tallies.done += 1
                            new_bucket = buckets.done
                        j.move_to(new_bucket)
                    else:
                        # If we moved from queued to pending...
                        if (bucket == buckets.queued) and j.pending:
                            tallies.pending += 1
                            j.move_to(buckets.pending)
                        j.choose_next_check_time()
                        
                finally:
                    # Rewrite job file to capture events that happened.
                    j.aged = 0
                    j.save()
                    # Guarantee that the job is not marked busy when we leave.
                    j.set_busy(False)
            else:
                tallies.idle += 1
        except:
            traceback.print_exc()
            continue
        
class tallies:
    def __init__(self):
        self.payloads = 0
        self.retries = 0
        self.delegates_changed = 0
        self.done = 0
        self.failed = 0
        self.pending = 0
        self.idle = 0
        self.skipped = 0
    def is_empty(self):
        return self.payloads + self.retries + self.delegates_changed + self.done + self.failed + self.pending + self.idle + self.skipped == 0
    def __str__(self):
        return '''In this pass:
  %d payloads were transferred
  %d retries were scheduled
  %d delegated jobs changed status
  %d jobs finished successfully
  %d jobs failed
  %d jobs had portions delegated to remote node(s)
  %d jobs were idle awaiting maturity
  %d jobs were skipped because they were malformed
''' % (self.payloads, self.retries, self.delegates_changed, self.done, self.failed, self.pending, self.idle, self.skipped)


def more(quiet=False):
    print('\nChecking for more dis work on %s.' % time.asctime())
    
    # Keep track of what work we do.
    tally = tallies()
    
    # Each time we interact with a remote host that has to run cascaded jobs,
    # we want to guarantee that it has the latest version of this script, to
    # keep our software world automatically synchronized. However, we don't
    # want to update the same host more than once in a single call to more(),
    # so keep track of which remote hosts we've synced already.
    updated_hosts = {}
    
    # We handle the pending bucket before the queued bucket, because if we
    # do it in the opposite order, the same bucket may be processed twice.
    handle_bucket('pending', tally, updated_hosts)
    handle_bucket('queued', tally, updated_hosts)
    
    if quiet and tally.is_empty():
        print('  ...nothing to do.')
        return
    print(tally)
    

def status(which_buckets=None):
    if not which_buckets:
        which_buckets = buckets.active
    else:
        which_buckets = [which_buckets]
    for bucket in which_buckets:
        for id in buckets.ids_of_jobs_in(bucket):
            try:
                j = job.load(buckets.path_for_job(bucket, id))
            except:
                traceback.print_exc()
                print('%s/%s is unparseable.' % (bucket, id))
                continue
            print('\n%s: %s' % (id, bucket))
            src = j.sources[0].path
            if len(j.sources) > 1:
                src += ' + %d more' % len(srcs) - 1
            print('  %s --> %s' % (src, j.target))
            print('  hosts  : %s' % j.hosts)
            print('  done   : %s' % j.done)
            print('  failed : %s' % j.failed)
            print('  stopped: %s' % j.stopped)
            if bucket in buckets.active:
                print('  pending: %s' % j.pending)
                print('  last check time: %s' % friendly_time(j.last_check_time, zero='never'))
                next = friendly_time(j.next_check_time, zero='asap')
                if j.next_check_time <= time.time():
                    next += ' (NOW)'
                print('  next check time: %s' % next)
            print('  log:')
            if j.events:
                i = 0
                while i < 5:
                    print('    %s' % j.events[i])
                    i += 1
                    if i == len(j.events):
                        break
            print('')


def find_job(id_fragment):
    id_fragment = id_fragment.lower()
    for bucket in buckets.all:
        ids = buckets.ids_of_jobs_in(bucket)
        for id in ids:
            if id.lower().startswith(id_fragment):
                return id, bucket
    return None, None


def flush(which_buckets=None):
    if not which_buckets:
        which_buckets = buckets.all
    for bucket in which_buckets:
        folder = buckets.folder_for(bucket)
        for item in os.listdir(folder):
            os.remove(os.path.join(folder, item))
        print('Flushed %s' % bucket)


def stop(id_fragment, second_try=False):
    id, bucket = find_job(id_fragment)
    if id:
        if bucket not in buckets.active:
            print('Job %s is already %s.' % (id, bucket))
        else:
            j = job.load(buckets.path_for_job(bucket, id))
            j.move_to(buckets.stopped)
        return True
    
    # In case we failed to find an id because the item was busy...
    if not second_try:
        time.sleep(5)
        if not stop(id_fragment, True):
            complain("Unable to find job %s..." % id_fragment)


def reset(id_fragment):
    id, bucket = find_job(id_fragment)
    if not id:
        complain("Unable to find job %s..." % id_fragment)
    if bucket == buckets.done:
        print('Job %s is already done.' % id)
        
    job_path = buckets.path_for_job(bucket, id)
    j = job.load(job_path)
    j.retries = {}
    j.done = j.failed = j.pending = j.stopped = j.events = []
    j.next_check_time = 0
    j.log('Reset.')
    j.save(job_path)
    j.move_to(buckets.queued)
    # If the job was already in the queued bucket, then the .move_to method
    # won't generate a message... so add one ourselves.
    if bucket == buckets.queued:
        print('Job %s has been reset.' % id)
        
        
def age(id_fragment):
    id, bucket = find_job(id_fragment)
    if not id:
        complain("Unable to find job %s..." % id_fragment)
    if bucket not in buckets.active:
        complain('Job %s is already %s.' % (id, bucket))
        
    job_path = buckets.path_for_job(bucket, id)
    j = job.load(job_path)
    j.next_check_time = 0
    j.aged = 1
    j.log('Aged job so it is ready to attempt again.')
    j.save(job_path)
    print('Job %s has been aged so it is ready to attempt again.' % id)


def make_target_list():
    x = targets.keys()
    if not x:
        x = "undefined"
    if len(x) == 1:
        x = '"%s"' % x[0]
    elif (len(x)) == 2:
        x = '"%s" and "%s"' % (x[0], x[1])
    else:
        x = '"' + '", "'.join(x) + '"'
        last_comma = x.rfind(',')
        x = x[:last_comma + 1] + ' and ' + x[last_comma + 2:]
    return x


if __name__ == '__main__':
    args = sys.argv[1:]
    if args:
        verb = args[0]
        args = args[1:]
    if globals().has_key(verb):
        func = globals()[verb]
        func(*args)
    else:
        help(args)
